{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if PyTorch recognizes GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create function to extract frames from a video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.opencv.org/4.x/d8/dfe/classcv_1_1VideoCapture.html\n",
    "\n",
    "def frame_extract(filepath, max_frames, frame_size):\n",
    "    frames = []\n",
    "\n",
    "    # begin reading from video\n",
    "    capture = cv2.VideoCapture(filepath)\n",
    "    if not capture.isOpened():\n",
    "        print(\"Cannot open file\")\n",
    "        exit()\n",
    "    while True:\n",
    "        retval, image = capture.read()\n",
    "        if not retval:\n",
    "            break\n",
    "        image = cv2.resize(image, frame_size)\n",
    "        frames.append(image)\n",
    "\n",
    "    # stop reading from video\n",
    "    capture.release()\n",
    "\n",
    "    # pad video by repeating animation until all samples are of equal length Holding -> 209\n",
    "    # Pad video by repeating frames cyclically if it's shorter than max_frames\n",
    "    while len(frames) < max_frames:\n",
    "        num_padded = max_frames - len(frames)\n",
    "        repeat_frames = frames[:num_padded]  # Repeat from the start\n",
    "        frames.extend(repeat_frames)\n",
    "\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that the correct amount of frames are extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Pointing_Day_Plain_front_30fps.mkv'\n",
    "test_filepath = './dataset/Pointing/Day/Plain/' + filename\n",
    "max_frames = 210 # longest animation is Holding_Something_In_Pain at 209 frames\n",
    "frame_size = (104, 104)\n",
    "test_frames = frame_extract(test_filepath, max_frames, frame_size)\n",
    "print(test_frames.shape)# prints (num frames, height, width, channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display example frames from image sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_frame1 = 48\n",
    "example_frame = cv2.cvtColor(test_frames[ex_frame1], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(example_frame)\n",
    "plt.title(f'Frame {ex_frame1} of {filename}')\n",
    "plt.show()\n",
    "\n",
    "ex_frame2 = 80\n",
    "example_frame2 = cv2.cvtColor(test_frames[ex_frame2], cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(example_frame2)\n",
    "plt.title(f'Frame {ex_frame2} of {filename}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "class DistressActionDataset(Dataset):\n",
    "    def __init__(self, root, max_frames, frame_size):\n",
    "        self.root = root\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_size = frame_size\n",
    "\n",
    "        #self.transform = transform\n",
    "        self.label_map = {\n",
    "            \"Holding_Something_In_Pain\": 0,\n",
    "            \"Injured_Walk\": 1,\n",
    "            \"Jumping\": 2,\n",
    "            \"Pointing\": 3,\n",
    "            \"Running\": 4,\n",
    "            \"Waving\": 5,\n",
    "            \"Blowing_a_kiss\": 6,\n",
    "            \"Greeting\": 7,\n",
    "            \"Rumba_Dancing\": 8,\n",
    "            \"Salute\": 9,\n",
    "            \"Silly_Dancing\": 10,\n",
    "            \"Sitting\": 11\n",
    "        }\n",
    "\n",
    "        # add all mp4's to 'videos', and appropriate label (as filepaths) to 'labels'\n",
    "        # Distress Labels: Holding_Something_In_Pain, Injured_Walk, Jumping, Pointing, Running, Waving\n",
    "        # Non-Distress Labels: Blowing_a_kiss, Greeting, Rumba_Dancing, Salute, Silly_Dancing, Sitting\n",
    "        \n",
    "        for label_folder in os.listdir(root):\n",
    "            label_filepath = os.path.join(root, label_folder)\n",
    "            # Times: Day, Night\n",
    "            for time_folder in os.listdir(label_filepath):\n",
    "                time_filepath = os.path.join(label_filepath, time_folder)\n",
    "                # Environments: Rural, Plain, Grassy\n",
    "                for env_folder in os.listdir(time_filepath):\n",
    "                    env_filepath = os.path.join(time_filepath, env_folder)\n",
    "                    # Direction: Right, Left, Front, Back\n",
    "                    for video in os.listdir(env_filepath):\n",
    "                        if video.endswith(\".mkv\"):\n",
    "                            video_file = os.path.join(env_filepath, video)\n",
    "                            print(env_filepath)\n",
    "                            data_frames = frame_extract(video_file, self.max_frames, self.frame_size)\n",
    "                            self.data.append(data_frames)\n",
    "                            self.labels.append(self.label_map[label_folder])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # put frames on tensor in (frames, channels, height, width) order\n",
    "        # normalize pixel values\n",
    "        frames = torch.from_numpy(video).permute(0, 3, 1, 2).float() / 255.0\n",
    "        # Apply transform to all frames in video\n",
    "        # if self.transform:\n",
    "        #     for i in range(len(frames)):\n",
    "        #         frames[i] = self.transform(frames[i])\n",
    "        \n",
    "        # convert to tensor\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return frames, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained CNN models - https://pytorch.org/vision/0.9/models.html\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class DistressClassifier(nn.Module):\n",
    "    def __init__(self, classes=12):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet18(pretrained=True)\n",
    "\n",
    "        # freeze cnn base model parameters\n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.resnet.fc = nn.Sequential(nn.Linear(self.resnet.fc.in_features, 128))\n",
    "\n",
    "        # unfreeze cnn model's fully connected layers\n",
    "        for param in self.resnet.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # TODO: can stack multiple LSTM cells with 'num_layers' arg\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=64)\n",
    "        self.fc1 = nn.Linear(64, 48)\n",
    "        self.fc2 = nn.Linear(48, classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = None\n",
    "\n",
    "        for t in range(x.size(1)):\n",
    "            with torch.no_grad():\n",
    "                resnet_x = self.resnet(x[:, t])  \n",
    "            # pass latent representation of frame through lstm and update hidden state\n",
    "            _, hidden = self.lstm(resnet_x.unsqueeze(0), hidden)         \n",
    "\n",
    "        # get the last hidden state\n",
    "        resnet_x = self.fc1(hidden[0][-1])\n",
    "        resnet_x = F.relu(resnet_x)\n",
    "        resnet_x = self.fc2(resnet_x)\n",
    "\n",
    "        return resnet_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.transforms import v2\n",
    "\n",
    "# augment = v2.Compose([\n",
    "#     v2.RandomHorizontalFlip(),\n",
    "#     v2.RandomVerticalFlip(),\n",
    "#     v2.GaussianNoise()\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Dataset into Training and Test portions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 210 # longest animation is Holding_Something_In_Pain at 209 frames\n",
    "frame_size = (52, 52)\n",
    "dataset_path = './dataset'\n",
    "\n",
    "dataset = DistressActionDataset(dataset_path, max_frames, frame_size)\n",
    "\n",
    "# 80% of dataset set aside for training, 20% for test\n",
    "train_len = int(len(dataset)*0.8)      \n",
    "train_set, test_set = random_split(dataset, [train_len, len(dataset)-train_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DataLoader, Optimizer, Loss Function, and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistressClassifier()\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This training loop is modified from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "\n",
    "def train_model(model, train_loader, optimizer, loss_fn, epoch_count):\n",
    "    for epoch_index in range(epoch_count):\n",
    "        print(f\"Epoch {epoch_index + 1}/{epoch_count}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # get sample, move to device\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # make predictions for batch\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # compute loss and gradients\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # adjust learning weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # gather training data and print periodically\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            print(f\"  Batch {i + 1} loss: {running_loss:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "        print(f\"Epoch {epoch_index + 1} complete. Last batch loss: {running_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 9\n",
    "train_model(model, train_dataloader, optimizer, loss_fn, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(inputs)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the network: {100 * correct / total} %')\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = evaluate_model(model, test_dataloader)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/model_weights8-{accuracy:.1f}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights (if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = './models/model_weights8-3.4.pth'\n",
    "model.load_state_dict(torch.load(saved_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_allclass(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.extend(predicted.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    report = classification_report(all_labels, all_predictions, labels=list(range(12)), target_names=[\n",
    "        \"Holding_Something_In_Pain\", \n",
    "        \"Injured_Walk\",\n",
    "        \"Jumping\",\n",
    "        \"Pointing\",\n",
    "        \"Running\",\n",
    "        \"Waving\",\n",
    "        \"Blowing_a_kiss\",\n",
    "        \"Greeting\",\n",
    "        \"Rumba_Dancing\",\n",
    "        \"Salute\",\n",
    "        \"Silly_Dancing\",\n",
    "        \"Sitting\"\n",
    "    ], zero_division=0)\n",
    "    conf_mat = confusion_matrix(all_labels, all_predictions, labels=list(range(12)))\n",
    "    return accuracy, report, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of binary classification (distress vs non distress classes)\n",
    "def eval_distress_binary(model, test_loader, device):\n",
    "    # define distress vs non distress label maps\n",
    "    distress_labels = {0, 1, 2, 3, 4, 5}  \n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # remap 12 class labels into binary classification\n",
    "            binary_labels = torch.zeros_like(labels)  # Default all to 0\n",
    "            binary_labels[torch.isin(labels, torch.tensor(list(distress_labels)).to(device))] = 1\n",
    "            \n",
    "            # forward pass through model, obtain predictions\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # map predictions to binary classification problem\n",
    "            binary_predictions = torch.zeros_like(predicted)\n",
    "            binary_predictions[torch.isin(predicted, torch.tensor(list(distress_labels)).to(device))] = 1\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (binary_predictions == binary_labels).sum().item()\n",
    "            all_predictions.extend(binary_predictions.cpu().tolist())\n",
    "            all_labels.extend(binary_labels.cpu().tolist())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    report = classification_report(all_labels, all_predictions, labels=list(range(2)), target_names=[\n",
    "        \"Non Distress\", \n",
    "        \"Distress\"\n",
    "    ], zero_division=0)\n",
    "    conf_mat = confusion_matrix(all_labels, all_predictions, labels=list(range(2)))\n",
    "    return accuracy, report, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_distress(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Remap labels: 0-5 stay as is, 6-11 are remapped to 6\n",
    "            remapped_labels = labels.clone()\n",
    "            remapped_labels[labels >= 6] = 6\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Map predictions in the same way as remapped labels\n",
    "            remapped_predictions = predicted.clone()\n",
    "            remapped_predictions[predicted >= 6] = 6\n",
    "\n",
    "            total += remapped_labels.size(0)\n",
    "            correct += (remapped_predictions == remapped_labels).sum().item()\n",
    "            all_predictions.extend(remapped_predictions.cpu().tolist())\n",
    "            all_labels.extend(remapped_labels.cpu().tolist())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    report = classification_report(all_labels, all_predictions, labels=list(range(7)), target_names=[\n",
    "        \"Holding_Something_In_Pain\", \n",
    "        \"Injured_Walk\", \n",
    "        \"Jumping\", \n",
    "        \"Pointing\", \n",
    "        \"Running\", \n",
    "        \"Waving\", \n",
    "        \"Non_Distress\"\n",
    "    ], zero_division=0)\n",
    "    conf_mat = confusion_matrix(all_labels, all_predictions, labels=list(range(7)))\n",
    "\n",
    "    return accuracy, report, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model as a binary classification problem\n",
    "# 'Detect Distress vs Non-distress'\n",
    "accuracy, report, conf_mat1 = eval_distress_binary(model, test_dataloader, device)\n",
    "print('Binary classification: ')\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "# Evaluate the model as a multiclass classification problem across distress classes\n",
    "# 'Detect Distress type IF Distress action'\n",
    "accuracy, report, conf_mat2 = evaluate_model_distress(model, test_dataloader, device)\n",
    "print('Multiclass classification: ')\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "\n",
    "print('-------------------------------------------------------')\n",
    "\n",
    "# Evaluate the model as a multiclass classification problem across all classes\n",
    "# 'Detect Action'\n",
    "accuracy, report, conf_mat3 = evaluate_model_allclass(model, test_dataloader, device)\n",
    "print('Multiclass classification: ')\n",
    "print(f'Accuracy: {accuracy:.2f}%')\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, class_labels):\n",
    "    print('\\n')\n",
    "    sns.heatmap(\n",
    "        conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "        xticklabels=class_labels, yticklabels=class_labels\n",
    "    )\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Evaluate the model as a binary classification problem\n",
    "# 'Detect Distress vs Non-distress'\n",
    "class_labels1 = [\"Non-Distress\", \"Distress\"]\n",
    "plot_confusion_matrix(conf_mat1, class_labels1)\n",
    "\n",
    "# Evaluate the model as a multiclass classification problem across distress classes\n",
    "# 'Detect Distress type IF Distress action'\n",
    "class_labels2 = [\"Holding\", \"Injured Walk\", \"Jumping\", \\\n",
    "                \"Pointing\", \"Running\", \"Waving\", \"Non Distress\"]\n",
    "plot_confusion_matrix(conf_mat2, class_labels2)\n",
    "\n",
    "# Evaluate the model as a multiclass classification problem across all classes\n",
    "# 'Detect Action'\n",
    "class_labels3 = [\"Holding\", \"Injured Walk\", \"Jumping\",\n",
    "                \"Pointing\", \"Running\", \"Waving\", \"Blowing Kiss\", \"Greeting\", \n",
    "                \"Rumba Dance\", \"Salute\", \"Silly Dance\", \"Sitting\"]\n",
    "plot_confusion_matrix(conf_mat3, class_labels3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc678",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
